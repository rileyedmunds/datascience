{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 346,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "' \\nRandom Forests\\n\\n    Take a bunch of decision trees trained on portions of the training set, \\n    and overfit to their portions. Then random forests are the technique to\\n    combine these decision trees in order to reduce variance (overfitting).\\n\\n\\n    Bagging (Bootstrap aggregating):\\n    randomly sample from the trainining set and sample some number b of subsets with replacement.\\n    then train decision trees on each subset B (typically 100 to 1000 trees)\\n    Predictions can be made to rediuce variance without adding bias by:\\n        -continuous: average the outputs\\n        -discrete: take majority vote\\n    Note: bagging assumes no correlation between models. But there is some!!!\\n\\n\\n    Stacking:\\n    take output of each model in a bucker of models ensemble, \\n    and pass it as input to a perceptron that combines the outputs to find a best result.\\n    This is demonstrably better than cross-validation selection.\\n\\n\\nENSEMBLES\\n\\n    Technically, ensemble means generating and averaging different hypotheses from the same model\\n    and multiple classifier systems covers hybrid systems that use diffrent learners.\\n\\n    Idea:\\n    You can overfit on a bunch of models, and then ensemble them.\\n        This is better than dumbing down models in an effort to reduce overfitting\\n\\n    -How many hypotheses should we combine in the ensemble?\\n        There is a theorem, \"the law of diminishing returns in ensemble contruction\" says \\n        that number of classifiers = number of classes is optimal!\\n\\n\\n    -Cross-validation selection (uses cross validation to select)\\n        \"Bucket of models\" method where not sure which model works best.\\n\\n\\n    ALGORITHM\\n    Pseudocode:\\n        write a bunch of models\\n        break training data into two A and B\\n        for each model:\\n            train on A test on B\\n            record accuracy on B\\n        Once done, find model with best accuracy on B and use that one!\\n\\n'"
      ]
     },
     "execution_count": 346,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "''' \n",
    "Random Forests\n",
    "\n",
    "    Take a bunch of decision trees trained on portions of the training set, \n",
    "    and overfit to their portions. Then random forests are the technique to\n",
    "    combine these decision trees in order to reduce variance (overfitting).\n",
    "    \n",
    "    ALGORITHM:\n",
    "       -take decision tree and sample from the dataset b times for b decision trees.\n",
    "        -then train them all\n",
    "        -then choose output by majority voting for discrete classes, mean for regression problems\n",
    "\n",
    "\n",
    "    Bagging (Bootstrap aggregating):\n",
    "    randomly sample from the trainining set and sample some number b of subsets with replacement.\n",
    "    then train decision trees on each subset B (typically 100 to 1000 trees)\n",
    "    Predictions can be made to rediuce variance without adding bias by:\n",
    "        -continuous: average the outputs\n",
    "        -discrete: take majority vote\n",
    "    Note: bagging assumes no correlation between models. But there is some!!!\n",
    "\n",
    "\n",
    "    Stacking:\n",
    "    take output of each model in a bucker of models ensemble, \n",
    "    and pass it as input to a perceptron that combines the outputs to find a best result.\n",
    "    This is demonstrably better than cross-validation selection.\n",
    "\n",
    "\n",
    "ENSEMBLES\n",
    "\n",
    "    Technically, ensemble means generating and averaging different hypotheses from the same model\n",
    "    and multiple classifier systems covers hybrid systems that use diffrent learners.\n",
    "\n",
    "    Idea:\n",
    "    You can overfit on a bunch of models, and then ensemble them.\n",
    "        This is better than dumbing down models in an effort to reduce overfitting\n",
    "\n",
    "    -How many hypotheses should we combine in the ensemble?\n",
    "        There is a theorem, \"the law of diminishing returns in ensemble contruction\" says \n",
    "        that number of classifiers = number of classes is optimal!\n",
    "\n",
    "\n",
    "    -Cross-validation selection (uses cross validation to select)\n",
    "        \"Bucket of models\" method where not sure which model works best.\n",
    "\n",
    "\n",
    "    ALGORITHM\n",
    "    Pseudocode:\n",
    "        write a bunch of models\n",
    "        break training data into two A and B\n",
    "        for each model:\n",
    "            train on A test on B\n",
    "            record accuracy on B\n",
    "        Once done, find model with best accuracy on B and use that one!\n",
    "\n",
    "'''\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 348,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "-- iris.csv found locally\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  5,\n",
       "        5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "        5,  5,  9,  5,  5,  5,  5,  5,  5,  7,  5,  5,  5,  5,  5,  7,  5,\n",
       "        5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5, 10, 10,\n",
       "       10, 10, 10, 10,  6, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        7, 10, 10, 10, 10, 10, 10,  9, 10, 10,  7, 10, 10, 10,  7,  7, 10,\n",
       "       10, 10,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10])"
      ]
     },
     "execution_count": 348,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#DECISION TREE CODE\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz\n",
    "\n",
    "def get_iris_data():\n",
    "    \"\"\"Get the iris data, from local csv or pandas repo.\"\"\"\n",
    "    if os.path.exists(\"iris.csv\"):\n",
    "        print(\"-- iris.csv found locally\")\n",
    "        df = pd.read_csv(\"iris.csv\", index_col=0)\n",
    "    else:\n",
    "        print(\"-- trying to download from github\")\n",
    "        fn = \"https://raw.githubusercontent.com/pydata/pandas/\" + \\\n",
    "             \"master/pandas/tests/data/iris.csv\"\n",
    "        try:\n",
    "            df = pd.read_csv(fn)\n",
    "        except:\n",
    "            exit(\"-- Unable to download iris.csv\")\n",
    "\n",
    "        with open(\"iris.csv\", 'w') as f:\n",
    "            print(\"-- writing to local iris.csv file\")\n",
    "            df.to_csv(f)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = get_iris_data()\n",
    "\n",
    "#encoding names to integers\n",
    "def encode_target(df, target_column):\n",
    "    \"\"\"Add column to df with integers for the target.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    df -- pandas DataFrame.\n",
    "    target_column -- column to map to int, producing\n",
    "                     new Target column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_mod -- modified DataFrame.\n",
    "    targets -- list of target names.\n",
    "    \"\"\"\n",
    "    df_mod = df.copy()\n",
    "    targets = df_mod[target_column].unique()\n",
    "    map_to_int = {name: n for n, name in enumerate(targets)}\n",
    "    df_mod[\"Target\"] = df_mod[target_column].replace(map_to_int)\n",
    "\n",
    "    return (df_mod, targets)\n",
    "\n",
    "\n",
    "#print out data\n",
    "df2, targets = encode_target(df, \"Name\")\n",
    "\n",
    "#print feature column names\n",
    "features = list(df2.columns[:4])\n",
    "\n",
    "#writing model\n",
    "y = df2[\"Target\"]\n",
    "X = df2[features]\n",
    "dt = DecisionTreeClassifier(min_samples_split=20, random_state=99) #abstracted away\n",
    "dt.fit(X, y)\n",
    "\n",
    "#output matrix\n",
    "dt.apply(df2[features])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#######################\n",
    "#### RANDOM FOREST ####\n",
    "#######################\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 349,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Building off decision tree!\n",
    "\n",
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "#running discrete random forests on iris dataset (majority voting) with 100 trees\n",
    "\n",
    "def sample(num_sample):\n",
    "    '''gathers n samples (data and label) from the dataset with replacement'''\n",
    "    sampled_data = data.sample(n=num_sample, replace=True)\n",
    "    sampled_label = label.ix[sampled_data.index.values.tolist()]\n",
    "    return sampled_data, sampled_label\n",
    "\n",
    "def train_tree(data, label):\n",
    "    '''trains a decision tree on some data'''\n",
    "    #writing model\n",
    "    y = label_1\n",
    "    X = data_1\n",
    "    dt = DecisionTreeClassifier(min_samples_split=5, random_state=99) #abstracted away\n",
    "    dt.fit(X, y)\n",
    "    return dt\n",
    "\n",
    "def eval_tree(tree, data):\n",
    "    '''runs the tree on some data and returns the output'''\n",
    "    result = tree.apply(data)\n",
    "    return result\n",
    "    \n",
    "def combine_discrete(outputs):\n",
    "    '''returns majority vote of outputs (array of results)'''\n",
    "    mode = stats.mode(outputs)[0]\n",
    "    return mode"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 350,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#data is found in df2[\"Target\"] and df2[features]\n",
    "\n",
    "label = df2[\"Target\"]\n",
    "data = df2[features]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 351,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#some constants\n",
    "num_trees = 100\n",
    "num_samples_per_tree = 15\n",
    "\n",
    "#creating decision trees for each sample and storing them in an array of classifiers\n",
    "classifiers = []\n",
    "\n",
    "#creating one dt and test it, then store it in the array.\n",
    "data_1, label_1 = sample(15)\n",
    "tree_1 = train_tree(data_1, label_1)\n",
    "\n",
    "classifiers += [tree_1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 352,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#working for one. now for all the trees!\n",
    "classifiers = []\n",
    "\n",
    "#train num_trees decision trees!\n",
    "for i in range(num_trees):\n",
    "    data_i, label_i = sample(15)\n",
    "    tree_i = train_tree(data_i, label_i)\n",
    "    classifiers += [tree_i]\n",
    "    tree_i = 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 353,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#combining outputs from decision trees evaluated on all data, tested on two\n",
    "results = []\n",
    "\n",
    "for tree in classifiers:\n",
    "    results += [np.array(eval_tree(tree, data))]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 354,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2, 2,\n",
       "        2, 2, 2, 2, 2, 2, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3,\n",
       "        3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 3, 4, 3, 4, 4, 4, 4, 3, 4, 4, 4,\n",
       "        3, 4, 4, 3, 3, 4, 4, 4, 4, 3, 4, 3, 4, 3, 4, 4, 3, 3, 4, 4, 4, 4,\n",
       "        4, 3, 4, 4, 4, 4, 3, 4, 4, 3, 3, 4, 4, 3, 3, 3, 4, 3]])"
      ]
     },
     "execution_count": 354,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#result from majority vote\n",
    "combine_discrete(outputs=results)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
