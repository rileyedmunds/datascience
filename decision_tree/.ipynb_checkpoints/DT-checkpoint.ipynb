{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "collapsed": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'\\nThey are constructed with \\'training data\\' (supervised), then they can be evaluated on new data. \\n\\n\"The preferred sequence of attributes to investigate to most rapidly narrow down the state of X\"\\n\\nDecision trees prone to overfitting: so we prune them!\\n    (this involves replacing some next-to-leave nodes and their children with a single node)\\nDeals with continuous/discrete split by thresholding continuous data into discrete bins\\nUses information gain to construct tree \\n    (KL divergence between prior and posterior when splitting).\\n    IOW finds highest KL divergence between distributions split\\n    between branches of the tree to decide where to split the tree!\\n    \\n    \\n    \\nNames: \\nCART Classification and Regression Trees \\n    These have binary splits on the nodes, not multiclass\\nRegression Tree: decision tree where target variables can take on continuous variables\\n\\n\\n\\n\\nAlgorithm implementation:\\n1. Define some cost function\\n    -Variance reduction\\n    -Information gain (KL divergence)\\n    -Gini impurity\\n2. At each step in the tree, grid search all splits to find one that minimizes the cost function\\n3. Continue until hit some threshold number of splits, or reduced tree to leaves (single nodes at ends)\\n4. If necessary, prune to reduce overfitting\\n\\n\\n\\n#They can be inspected! Not a black box model!\\n\\nNote bias-variance tradeoff:\\n\\nUnderfitting: High bias, low variance (captures wrong \\'location\\' of data)\\nOverfitting: Low bias, high variance (captures noise in data)\\n\\n'"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#decision trees:\n",
    "'''\n",
    "They are constructed with 'training data' (supervised), then they can be evaluated on new data. \n",
    "\n",
    "\"The preferred sequence of attributes to investigate to most rapidly narrow down the state of X\"\n",
    "\n",
    "Decision trees prone to overfitting: so we prune them!\n",
    "    (this involves replacing some next-to-leave nodes and their children with a single node)\n",
    "Deals with continuous/discrete split by thresholding continuous data into discrete bins\n",
    "Uses information gain to construct tree \n",
    "    (KL divergence between prior and posterior when splitting).\n",
    "    IOW finds highest KL divergence between distributions split\n",
    "    between branches of the tree to decide where to split the tree!\n",
    "    \n",
    "    \n",
    "    \n",
    "Names: \n",
    "CART Classification and Regression Trees \n",
    "    These have binary splits on the nodes, not multiclass\n",
    "Regression Tree: decision tree where target variables can take on continuous variables\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "Algorithm implementation:\n",
    "1. Define some cost function\n",
    "    -Variance reduction\n",
    "    -Information gain (KL divergence)\n",
    "    -Gini impurity\n",
    "2. At each step in the tree, grid search all splits to find one that minimizes the cost function\n",
    "3. Continue until hit some threshold number of splits, or reduced tree to leaves (single nodes at ends)\n",
    "4. If necessary, prune to reduce overfitting\n",
    "\n",
    "\n",
    "\n",
    "#They can be inspected! Not a black box model!\n",
    "\n",
    "Note bias-variance tradeoff:\n",
    "\n",
    "Underfitting: High bias, low variance (captures wrong 'location' of data)\n",
    "Overfitting: Low bias, high variance (captures noise in data)\n",
    "\n",
    "'''"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#Implementation w/ sklearn\n",
    "\n",
    "from __future__ import print_function\n",
    "\n",
    "import os\n",
    "import subprocess\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.tree import DecisionTreeClassifier, export_graphviz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#pulling sample data\n",
    "def get_iris_data():\n",
    "    \"\"\"Get the iris data, from local csv or pandas repo.\"\"\"\n",
    "    if os.path.exists(\"iris.csv\"):\n",
    "        print(\"-- iris.csv found locally\")\n",
    "        df = pd.read_csv(\"iris.csv\", index_col=0)\n",
    "    else:\n",
    "        print(\"-- trying to download from github\")\n",
    "        fn = \"https://raw.githubusercontent.com/pydata/pandas/\" + \\\n",
    "             \"master/pandas/tests/data/iris.csv\"\n",
    "        try:\n",
    "            df = pd.read_csv(fn)\n",
    "        except:\n",
    "            exit(\"-- Unable to download iris.csv\")\n",
    "\n",
    "        with open(\"iris.csv\", 'w') as f:\n",
    "            print(\"-- writing to local iris.csv file\")\n",
    "            df.to_csv(f)\n",
    "\n",
    "    return df\n",
    "\n",
    "df = get_iris_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* df.head()\n",
      "   SepalLength  SepalWidth  PetalLength  PetalWidth         Name\n",
      "0          5.1         3.5          1.4         0.2  Iris-setosa\n",
      "1          4.9         3.0          1.4         0.2  Iris-setosa\n",
      "2          4.7         3.2          1.3         0.2  Iris-setosa\n",
      "3          4.6         3.1          1.5         0.2  Iris-setosa\n",
      "4          5.0         3.6          1.4         0.2  Iris-setosa\n",
      "\n",
      "* df.tail()\n",
      "     SepalLength  SepalWidth  PetalLength  PetalWidth            Name\n",
      "145          6.7         3.0          5.2         2.3  Iris-virginica\n",
      "146          6.3         2.5          5.0         1.9  Iris-virginica\n",
      "147          6.5         3.0          5.2         2.0  Iris-virginica\n",
      "148          6.2         3.4          5.4         2.3  Iris-virginica\n",
      "149          5.9         3.0          5.1         1.8  Iris-virginica\n",
      "\n"
     ]
    }
   ],
   "source": [
    "#printing some of the data\n",
    "print(\"* df.head()\", df.head(), sep=\"\\n\", end=\"\\n\\n\")\n",
    "print(\"* df.tail()\", df.tail(), sep=\"\\n\", end=\"\\n\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* iris types:\n",
      "['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n"
     ]
    }
   ],
   "source": [
    "#classes\n",
    "print(\"* iris types:\", df[\"Name\"].unique(), sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#encoding \n",
    "def encode_target(df, target_column):\n",
    "    \"\"\"Add column to df with integers for the target.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    df -- pandas DataFrame.\n",
    "    target_column -- column to map to int, producing\n",
    "                     new Target column.\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    df_mod -- modified DataFrame.\n",
    "    targets -- list of target names.\n",
    "    \"\"\"\n",
    "    df_mod = df.copy()\n",
    "    targets = df_mod[target_column].unique()\n",
    "    map_to_int = {name: n for n, name in enumerate(targets)}\n",
    "    df_mod[\"Target\"] = df_mod[target_column].replace(map_to_int)\n",
    "\n",
    "    return (df_mod, targets)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* df2.head()\n",
      "   Target         Name\n",
      "0       0  Iris-setosa\n",
      "1       0  Iris-setosa\n",
      "2       0  Iris-setosa\n",
      "3       0  Iris-setosa\n",
      "4       0  Iris-setosa\n",
      "\n",
      "* df2.tail()\n",
      "     Target            Name\n",
      "145       2  Iris-virginica\n",
      "146       2  Iris-virginica\n",
      "147       2  Iris-virginica\n",
      "148       2  Iris-virginica\n",
      "149       2  Iris-virginica\n",
      "\n",
      "* targets\n",
      "['Iris-setosa' 'Iris-versicolor' 'Iris-virginica']\n",
      "\n",
      "* features:\n",
      "['SepalLength', 'SepalWidth', 'PetalLength', 'PetalWidth']\n"
     ]
    }
   ],
   "source": [
    "#print out data\n",
    "df2, targets = encode_target(df, \"Name\")\n",
    "print(\"* df2.head()\", df2[[\"Target\", \"Name\"]].head(),\n",
    "      sep=\"\\n\", end=\"\\n\\n\")\n",
    "print(\"* df2.tail()\", df2[[\"Target\", \"Name\"]].tail(),\n",
    "      sep=\"\\n\", end=\"\\n\\n\")\n",
    "print(\"* targets\", targets, sep=\"\\n\", end=\"\\n\\n\")\n",
    "\n",
    "\n",
    "#print feature column names\n",
    "features = list(df2.columns[:4])\n",
    "print(\"* features:\", features, sep=\"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "###############\n",
    "#### MODEL ####\n",
    "###############"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "DecisionTreeClassifier(class_weight=None, criterion='gini', max_depth=None,\n",
       "            max_features=None, max_leaf_nodes=None, min_samples_leaf=1,\n",
       "            min_samples_split=20, min_weight_fraction_leaf=0.0,\n",
       "            presort=False, random_state=99, splitter='best')"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#writing model\n",
    "y = df2[\"Target\"]\n",
    "X = df2[features]\n",
    "dt = DecisionTreeClassifier(min_samples_split=20, random_state=99) #abstracted away\n",
    "dt.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#saving visualization\n",
    "def visualize_tree(tree, feature_names):\n",
    "    \"\"\"Create tree png using graphviz.\n",
    "\n",
    "    Args\n",
    "    ----\n",
    "    tree -- scikit-learn DecsisionTree.\n",
    "    feature_names -- list of feature names.\n",
    "    \"\"\"\n",
    "    with open(\"dt.dot\", 'w') as f:\n",
    "        export_graphviz(tree, out_file=f,\n",
    "                        feature_names=feature_names)\n",
    "\n",
    "    command = [\"dot\", \"-Tpng\", \"dt.dot\", \"-o\", \"dt.png\"]\n",
    "    try:\n",
    "        subprocess.check_call(command)\n",
    "    except:\n",
    "        exit(\"Could not run dot, ie graphviz, to \"\n",
    "             \"produce visualization\")\n",
    "        \n",
    "        \n",
    "visualize_tree(dt, features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([ 1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,\n",
       "        1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  1,  5,\n",
       "        5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,\n",
       "        5,  5,  9,  5,  5,  5,  5,  5,  5,  7,  5,  5,  5,  5,  5,  7,  5,\n",
       "        5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5,  5, 10, 10,\n",
       "       10, 10, 10, 10,  6, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10,\n",
       "        7, 10, 10, 10, 10, 10, 10,  9, 10, 10,  7, 10, 10, 10,  7,  7, 10,\n",
       "       10, 10,  9, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10, 10])"
      ]
     },
     "execution_count": 55,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#output matrix\n",
    "dt.apply(df2[features])"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [default]",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
